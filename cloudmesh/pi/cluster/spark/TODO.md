# Spark TO-DO file

Questions on how pieces of spark.py fit together

spark.py <https://github.com/cloudmesh/cloudmesh-pi-cluster/blob/master/cloudmesh/cluster/spark/spark.py>

README.md <https://github.com/cloudmesh/cloudmesh-pi-cluster/blob/master/cloudmesh/cluster/spark/README.md>

I very much appreciate you setting up the structure of the spark.py file
.  However, I don't understand how the pieces fit together.

1) how does **execute** function work with **setup** function?  Or are they
 separate?   An explanation with an opportunity to clarify on how the script is
  intended to work, could get me over a hump.

2) Was "arguments" put into the function as a placeholder or is it being
 used (examples in lines 32,34, 166)
 
 3) How do I test it when I'm ready?   If it's uploaded to cloudmesh
 , will cms pi find it?
 
 Questions on how to wrap up the semester project
 
 * What documentation is needed?  You mentioned a review with feedback and
  chance to revise.   Is May 1 the due date to get feedback?

Outstanding items:
1) Finish and test spark.py
2) Finalize README.md document with changes made to spark.py
3) Test the setup with common test script, e.g., how does Spark compare to
 Kubernetes?
4) Evaluate different setups with current Pi cluster (Hadoop, Kubernetes
).   Using new SD cards, burn different setups.